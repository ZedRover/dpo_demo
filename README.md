# DPO (Direct Preference Optimization) Implementation

å®Œæ•´å®ç°è®ºæ–‡ "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" ([arxiv:2305.18290](https://arxiv.org/abs/2305.18290))

## åŠŸèƒ½ç‰¹æ€§

- âœ… **æ ¸å¿ƒ DPO ç®—æ³•**: å®Œæ•´å®ç°è®ºæ–‡ä¸­çš„ DPO loss (æ–¹ç¨‹ 7)
- âœ… **æ•°æ®åŠ è½½**: æ”¯æŒ Anthropic/hh-rlhf æ•°æ®é›†
- âœ… **è®­ç»ƒæ¡†æ¶**: åŒ…å«å®Œæ•´çš„è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°
- âœ… **è®ºæ–‡æŒ‡æ ‡**: Reward-KL frontier, win rate ç­‰è¯„ä¼°æŒ‡æ ‡
- âœ… **å¯è§†åŒ–**: å¤ç°è®ºæ–‡ä¸­çš„ Figure 2 å’Œ Figure 3
- âœ… **æ–¹æ³•å¯¹æ¯”**: DPO vs Preferred-FT vs Best-of-N vs åŸºç¡€æ¨¡å‹
- âœ… **SLURM æ”¯æŒ**: å¯åœ¨è¶…ç®—é›†ç¾¤ä¸Šè¿è¡Œ

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ uv ç®¡ç†ç¯å¢ƒ
uv sync
```

### 2. æœ¬åœ°æµ‹è¯•è®­ç»ƒ

```bash
# ä½¿ç”¨å°æ¨¡å‹å¿«é€Ÿæµ‹è¯• (50æ­¥)
bash examples/local_test.sh
```

è¿™å°†:
- ä½¿ç”¨ GPT-2 æ¨¡å‹
- åœ¨ 100 ä¸ªæ ·æœ¬ä¸Šè®­ç»ƒ 50 æ­¥
- ä¿å­˜æ¨¡å‹åˆ° `outputs/local_test/`
- ç”Ÿæˆ reward-KL è¿½è¸ªæ•°æ®

### 3. ç”Ÿæˆå¯è§†åŒ–

```bash
# åˆ›å»ºè®ºæ–‡é£æ ¼çš„å›¾è¡¨
uv run python visualize.py \
    --tracker_path outputs/local_test/reward_kl_tracker.json \
    --output_dir plots
```

ç”Ÿæˆçš„å›¾è¡¨:
- `reward_kl_frontier.png` - Reward-KL è¾¹ç•Œ (Figure 2 å·¦)
- `training_curves.png` - è®­ç»ƒæ›²çº¿

## æ–¹æ³•å¯¹æ¯” (Figure 3)

å¤ç°è®ºæ–‡ Figure 3,å¯¹æ¯”å¤šç§æ–¹æ³•:

```bash
# ä¸€é”®è¿è¡Œæ‰€æœ‰å¯¹æ¯”å®éªŒ
bash examples/run_comparison.sh
```

è¿™å°†è®­ç»ƒå’Œè¯„ä¼°:
1. **DPO** - æˆ‘ä»¬çš„ä¸»è¦æ–¹æ³•
2. **Preferred-FT** - ä»…åœ¨ preferred completions ä¸Šå¾®è°ƒ
3. **Best of 128** - é‡‡æ ·128ä¸ªå“åº”é€‰æœ€ä½³
4. **Pythia-2.8B** - åŸºç¡€æ¨¡å‹

è¯¦ç»†è¯´æ˜è§ [COMPARISON_GUIDE.md](COMPARISON_GUIDE.md)

## å®Œæ•´è®­ç»ƒ

### åœ¨æœ¬åœ°è®­ç»ƒ

```bash
uv run python main.py \
    --model_name EleutherAI/pythia-410m \
    --beta 0.1 \
    --learning_rate 1e-6 \
    --max_steps 5000 \
    --batch_size 4 \
    --output_dir ./outputs/dpo_training \
    --sanity_check  # ä½¿ç”¨å°æ•°æ®é›†
```

### åœ¨ SLURM é›†ç¾¤è®­ç»ƒ

```bash
# ç¼–è¾‘ slurm_job.sh é…ç½®
# ç„¶åæäº¤ä»»åŠ¡
sbatch slurm_job.sh
```

## è¯„ä¼°

### è®¡ç®— Win Rate

```bash
uv run python evaluate.py \
    --model_path outputs/local_test/final_model \
    --num_samples 100 \
    --beta 0.1 \
    --output_dir ./eval_results
```

### å¯è§†åŒ–ç»“æœ

```bash
# å•ä¸ªæ–¹æ³•
uv run python visualize.py \
    --tracker_path outputs/dpo_training/reward_kl_tracker.json \
    --output_dir plots

# å¯¹æ¯”å¤šä¸ªæ–¹æ³•
uv run python visualize.py \
    --tracker_path outputs/dpo_training/reward_kl_tracker.json \
    --compare PPO:outputs/ppo_training/reward_kl_tracker.json \
    --output_dir plots
```

## é¡¹ç›®ç»“æ„

```
dpo_demo/
â”œâ”€â”€ dpo/                          # DPO æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loss.py                   # DPO loss å‡½æ•°
â”‚   â”œâ”€â”€ data.py                   # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”‚   â”œâ”€â”€ trainer.py                # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ metrics.py                # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ plotting.py               # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ main.py                       # è®­ç»ƒå…¥å£
â”œâ”€â”€ evaluate.py                   # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ visualize.py                  # å¯è§†åŒ–è„šæœ¬
â”œâ”€â”€ train_baselines.py            # è®­ç»ƒbaselineæ–¹æ³•
â”œâ”€â”€ evaluate_comparison.py        # å¤šæ–¹æ³•å¯¹æ¯”è¯„ä¼°
â”œâ”€â”€ config.py                     # é…ç½®æ–‡ä»¶
â”œâ”€â”€ slurm_job.sh                  # SLURM ä»»åŠ¡è„šæœ¬
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ local_test.sh             # æœ¬åœ°æµ‹è¯•è„šæœ¬
â”‚   â””â”€â”€ run_comparison.sh         # å¯¹æ¯”å®éªŒè„šæœ¬
â”œâ”€â”€ COMPARISON_GUIDE.md           # å¯¹æ¯”å®éªŒæŒ‡å—
â””â”€â”€ README.md                     # æœ¬æ–‡ä»¶
```

## æ ¸å¿ƒç®—æ³•

### DPO Loss

DPO ç›´æ¥ä¼˜åŒ–åå¥½,æ— éœ€æ˜¾å¼çš„reward model:

```python
L_DPO(Ï€_Î¸; Ï€_ref) = -E[(x,y_w,y_l)~D][log Ïƒ(Î² log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) - Î² log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))]
```

å…¶ä¸­:
- `y_w`: preferred completion
- `y_l`: rejected completion
- `Î²`: KLæƒ©ç½šå¼ºåº¦
- `Ï€_ref`: reference model (å†»ç»“)

### Implicit Reward

DPO éšå¼å­¦ä¹ ä¸€ä¸ªrewardå‡½æ•°:

```python
r(x, y) = Î² log(Ï€(y|x) / Ï€_ref(y|x))
```

## å®éªŒç»“æœ

è®ºæ–‡ä¸­çš„å…³é”®å‘ç°:

1. **Figure 2 (å·¦)**: DPO åœ¨ reward-KL frontier ä¸Šä¼˜äº PPO
2. **Figure 2 (å³)**: DPO åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šè¶…è¿‡ PPO
3. **Figure 3**: DPO åœ¨å¯¹è¯ä»»åŠ¡ä¸Šæ˜¯å”¯ä¸€æ”¹è¿› chosen responses çš„é«˜æ•ˆæ–¹æ³•

## é…ç½®è¯´æ˜

### å…³é”®è¶…å‚æ•°

- `--beta`: DPO æ¸©åº¦å‚æ•°,æ§åˆ¶ KL æƒ©ç½šå¼ºåº¦ (é»˜è®¤: 0.1)
- `--learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 1e-6)
- `--max_steps`: æœ€å¤§è®­ç»ƒæ­¥æ•°
- `--batch_size`: æ‰¹æ¬¡å¤§å°
- `--warmup_steps`: warmup æ­¥æ•° (é»˜è®¤: 150)

### æ¨¡å‹é€‰æ‹©

æ”¯æŒä»»ä½• HuggingFace æ¨¡å‹:
- å°æ¨¡å‹ (æµ‹è¯•): `gpt2`, `EleutherAI/pythia-410m`
- ä¸­æ¨¡å‹: `EleutherAI/pythia-1.4b`, `EleutherAI/pythia-2.8b`
- å¤§æ¨¡å‹: `EleutherAI/pythia-6.9b`, `facebook/opt-6.7b`

## è®ºæ–‡å¤ç°

### Figure 2 (å·¦): Reward-KL Frontier

```bash
# è®­ç»ƒå¤šä¸ª beta å€¼
for beta in 0.05 0.1 0.5 1.0; do
    uv run python main.py --beta $beta --output_dir outputs/beta_$beta
done

# ç»˜åˆ¶å¯¹æ¯”å›¾
uv run python visualize.py --compare beta_0.05:outputs/beta_0.05/...
```

### Figure 2 (å³): Summarization Win Rate

```bash
uv run python evaluate.py \
    --model_path outputs/dpo_model \
    --output_dir eval_results
```

### Figure 3: Method Comparison

```bash
bash examples/run_comparison.sh
```

## æ€§èƒ½ä¼˜åŒ–

### ä½¿ç”¨ GPU

```bash
uv run python main.py --device cuda
```

### å‡å°‘å†…å­˜å ç”¨

```bash
uv run python main.py \
    --batch_size 2 \
    --max_length 256 \
    --sanity_check
```

### å¯ç”¨æ··åˆç²¾åº¦

ç¼–è¾‘ `dpo/trainer.py`,ä½¿ç”¨ `torch.amp`

## å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠ?
A:
1. å‡å°‘ `--max_steps`
2. ä½¿ç”¨ `--sanity_check` (ä»…100ä¸ªæ ·æœ¬)
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
4. å¯ç”¨ GPU

### Q: Out of memory?
A:
1. å‡å°‘ `--batch_size`
2. å‡å°‘ `--max_length`
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†?
A:
ä¿®æ”¹ `dpo/data.py` ä¸­çš„ `load_hh_rlhf_dataset` å‡½æ•°

### Q: å¦‚ä½•ä½¿ç”¨ GPT-4 è¯„ä¼°?
A:
å‚è€ƒ `evaluate.py`,éœ€è¦ OpenAI API key

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬å®ç°,è¯·å¼•ç”¨åŸè®ºæ–‡:

```bibtex
@inproceedings{rafailov2023direct,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}
```

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## License

MIT License

## ç›¸å…³èµ„æº

- ğŸ“„ [åŸè®ºæ–‡](https://arxiv.org/abs/2305.18290)
- ğŸ¤— [Anthropic HH-RLHF æ•°æ®é›†](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- ğŸ“Š [è®ºæ–‡ç»“æœå¤ç°æŒ‡å—](COMPARISON_GUIDE.md)
